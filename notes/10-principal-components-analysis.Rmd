---
title: "Principal Components Analysis: Creating Linear Composites of Predictors"
description: | 
  A brief introduction to methods of combining correlated predictors. Example taken from @Chatterjee:2012.
author:
  - name: Andrew Zieffler 
    url: http://www.datadreaming.org/
date: "`r Sys.Date()`"
output:
  distill::distill_article:
    highlight: tango
bibliography: [epsy8264.bib]
csl: 'style/apa-single-spaced.csl'
---



<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<!-- A brief introduction to empirical diagnostics to detect collinearity. Example taken from @Chatterjee:2012. -->




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 6)
library(knitr)
library(kableExtra)
library(patchwork)
```

In 1964, the US Congress passed the Civil Rights Act and also ordered a survey of school districts to evaluate the availability of equal educational opportunity in public education. The results of this survey were reported on in @Coleman:1966 and @Mosteller:1972. The data in *equal-educational-opportunity.csv* consist of data taken from a random sample of 70 schools in 1965. The variables, which have all been mean-centered and standardized, include:

- `achievement`: Measurement indicating the student achievement level
- `faculty`: Measurement indicating the faculty's credentials
- `peer`: Measurement indicating the influence of peer groups in the school
- `school`: Measurement indicating the school facilities (e.g., building, teaching materials)

We will use these data to mimic one of the original regression analyses performed; examining whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credentials and peer influence.

```{r}
# Load libraries
library(broom)
library(car)
library(corrr)
library(tidyverse)

# Read in data
eeo = read_csv("~/Documents/github/epsy-8264/data/equal-education-opportunity.csv")
head(eeo)
```

The problem we faced from the last set of notes, was that the predictors in the model were collinear, so we encountered computational issues when trying to estimate the effects and standard errors. 

<br />


# Idea of Principal Components

If the *X*-matrix of the predictors were orthogonal, there would be no collinearity issues and we could easily estimate the effects and standard errors. This is, of course, not the case since the predictors are highly correlated. The idea of principal components analysis is to change the basis vectors (coordinate system) so that they are orthogonal. This is shown in the figure below in which we consider the predictor space composed of two of the predictors.

```{r fig.width=8, fig.height=4, fig.show="hold", fig.cap="LEFT: Faculty credential and peer influence measures shown in the coordinate system given by the $(1,0)$-$(0,1)$ basis. RIGHT: The coordinate system has been rotated based on the $(0.763,0.647)$-$(0.647,-0.762)$ basis.", echo=FALSE}
X = as.matrix(eeo[ , c("faculty", "peer")])

rot = eigen(crossprod(X))$vectors
new_X = data.frame(X %*% -rot)
names(new_X) = c("faculty", "peer")

# Original basis
p1 = ggplot(data = eeo, aes(x = faculty, y = peer)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "lightgrey") +
  geom_vline(xintercept = 0, color = "lightgrey") +
  geom_segment(x = 0, xend = 1, y = 0, yend = 0, arrow = arrow(length = unit(0.15,"cm")), color = "#E71D36", size = 1.25) +
  geom_segment(x = 0, xend = 0, y = 0, yend = 1, arrow = arrow(length = unit(0.15,"cm")), color = "#E71D36", size = 1.25) +
  scale_x_continuous(name = "Standardized measure of faculty's credentials", limits = c(-4, 4)) +
  scale_y_continuous(name = "Standardized measure of peer group influence", limits = c(-4, 4)) +
  theme_bw() +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )

# Rotated space
p2 = ggplot(data = eeo, aes(x = faculty, y = peer)) +
  geom_point()  +
  geom_abline(intercept = 0, slope = 0.8484635, color = "lightgrey") +
  geom_abline(intercept = 0, slope = -1.178601, color = "lightgrey") +
  geom_segment(x = 0, xend = 0.7625172, y = 0, yend = 0.6469680, arrow = arrow(length = unit(0.15,"cm")), color = "#E71D36", size = 1.25) +
  geom_segment(x = 0, xend = 0.6469680, y = 0, yend = -0.7625172, arrow = arrow(length = unit(0.15,"cm")), color = "#E71D36", size = 1.25) +
  scale_x_continuous(name = "Standardized measure of faculty's credentials", limits = c(-4, 4)) +
  scale_y_continuous(name = "Standardized measure of peer group influence", limits = c(-4, 4)) +
  theme_bw()

# Layout plots
p1 | p2
```

If we had considered all three predictors, the plot would be in three-dimensional space and we would need to rotate the coordinate system formed by the basis vectors (1, 0, 0)$-(0, 1, 0)-(0, 0, 1). With three dimensions, of course, we can now rotate in multiple directions. This idea can also be extended to *k*-dimensional space. (For now, we will continue to work with the predictor space defined by the `faculty` and `peer` predictors.)

Recall from our notes on vector geometry, that transforming the coordinates for a point to a new basis is a simple matter of pre-multiplying the vector of original coordinates by the matrix composed of the basis vectors. For example, the first observation had a `faculty` value of 0.608, and a `peer` value of 0.0351. 

```{r}
# Coordinates in original predictor space (row vector)
old = t(c(0.608, 0.0351))

# Matrix of new basis vectors
basis = matrix(c(0.7625172, 0.6469680, 0.6469680, -0.7625172), nrow = 2)

# Coordinates in rotated predictor space
old %*% basis 
```

Aside from producing an orthogonal basis, we also choose the principal components so that they maximize variance in the predictor space. For example, the direction of the first basis vector (i.e., the first principal component) is chosen to maximize the variation in the predictor space. This is essentially the direction of the major axis in the data ellipse. The second principal component is then chosen to maximize variance in an orthogonal direction to the first principal component. (With only two predictors, there is only one possible direction for the second principal component.) In our data ellipse this would be the direction of the minor axis. This continues until we exhaust the number of principal components.


```{r fig.width=6, fig.height=6, fig.cap="Data ellipse showing the directions of the principal components. The first principal component is in the direction of the major axis and the second principal component is in the direction of the minor axis.", echo=FALSE}
knitr::include_graphics("figs/pca-conceptual.png")
```


## Determining the Principal Components

Looking at the principal components in Figure 1 (RIGHT), it is clear that they lie in the subspace spanned by the original basis vectors. Because of this, we can express each principal component as a linear combination of the original predictor values. (To facilitate interpretations, all the predictors are typically standardized.) As such, we can write the first principal component as:

$$
\begin{split}
\begin{bmatrix}w_{1_1} \\ w_{1_2} \\w_{1_3} \\ \vdots \\w_{1_n} \end{bmatrix} &= c_{1_1}\begin{bmatrix}x_{1_1} \\ x_{1_2} \\x_{1_3} \\ \vdots \\x_{1_n} \end{bmatrix} + c_{2_1}\begin{bmatrix}x_{2_1} \\ x_{2_2} \\x_{2_3} \\ \vdots \\x_{2_n} \end{bmatrix} + \ldots + c_{k_1}\begin{bmatrix}x_{k_1} \\ x_{k_2} \\x_{k_3} \\ \vdots \\x_{k_n} \end{bmatrix} \\[1em]
\underset{n\times1}{\mathbf{W}_1} &= \underset{n\times k}{\mathbf{X}}~\underset{k\times1}{\mathbf{C}_1}
\end{split}
$$

where $\mathbf{W}_1$ is the basis vector defined by the first principal component, **X** is the matrix of predictor values, and $\mathbf{C}_1$ is a vector of coefficient weights that produce the desired value in $\mathbf{W}_1$. We can also compute the variance of $\mathbf{W}_1$ as:

$$
\begin{split}
S^2_{W_1} &= \dfrac{1}{n-1} \mathbf{W}_1^{\intercal} \mathbf{W}_1 \\[0.5em]
&= \dfrac{1}{n-1} \big(\mathbf{X}\mathbf{C}_1\big)^{\intercal}\big(\mathbf{X}\mathbf{C}_1\big) \\[0.5em]
&= \dfrac{1}{n-1} \mathbf{C}_1^{\intercal} \mathbf{X}^{\intercal}\mathbf{X}\mathbf{C}_1 \\[0.5em]
&= \mathbf{C}_1^{\intercal}\boldsymbol{\Sigma}_{XX}\mathbf{C}_1
\end{split}
$$

where $\mathbf{R}_{XX}$ is the variance--covariance matrix of the predictors (since $\boldsymbol{\Sigma}_{XX} = \dfrac{1}{n-1}\mathbf{X}^{\intercal}\mathbf{X}$).

Our goal is to maximize the variance, but in order to do that we need to constrain $\mathbf{C}_1^{\intercal}\mathbf{C}_1=1$. To maximize the variance under a constraint, we include a Lagrange multiplier in the formula for variance and then differentiate that with respect to $\mathbf{C}_1$ and the Lagrange multiplier; set those partial derivatives equal to 0; and solve. This produces two equations:

<aside>
Without the constraint, the variance could be made arbitrarily large by choosing large coefficients for $\mathbf{C}_1$.
</aside>

$$
\begin{split}
\big(\boldsymbol{\Sigma}_{XX} - \lambda_1\mathbf{I}\big)\mathbf{C}_1 = \mathbf{0} \\[0.5em]
\mathbf{C}_1^{\intercal}\mathbf{C}_1=1
\end{split}
$$
This gives solutions to $\mathbf{C}_1$ only when $\boldsymbol{\Sigma}_{XX} - \lambda_1\mathbf{I}$ is singular. This implies that:

- $\lambda_1$ is an eigenvalue of $\mathbf{R}_{XX}$; and
- $\mathbf{C}_1$ is the corresponding eigenvector (scaled so that $\mathbf{C}_1^{\intercal}\mathbf{C}_1=1$)

One problem is that there will be many eigenvalues/vectors to choose from since there will be *k* such pairs. From the equation based on the first partial derivative,

$$
\begin{split}
\big(\boldsymbol{\Sigma}_{XX} - \lambda_1\mathbf{I}\big)\mathbf{C}_1 &= \mathbf{0} \\[0.5em]
\boldsymbol{\Sigma}_{XX}\mathbf{C}_1 &= \lambda_1\mathbf{C}_1 
\end{split}
$$

Which we can substitute in to the variance formula (recall this is what we are maximizing).

$$
\begin{split}
S^2_{W_1} &= \mathbf{C}_1^{\intercal}\boldsymbol{\Sigma}_{XX}\mathbf{C}_1 \\[0.5em]
&= \mathbf{C}_1^{\intercal}\lambda_1\mathbf{C}_1 \\[0.5em]
&= \lambda_1 \mathbf{C}_1^{\intercal} \mathbf{C}_1 \\[0.5em]
&= \lambda_1 
\end{split}
$$

Maximizing the variance implies that we will want to choose the largest eigenvalue and corresponding eigenvector.

The second principal component is derived in a similar fashion, with the further constraint that it is orthogonal to the first principal component. (This process continues for the third, fourth, fifth, etc. principal components.) The math (not shown) indicates that maximizing the variance in the second principal component corresponds to choosing the second largest eigenvalue (and corresponding eigenvector) and so on. That is, finding the rotation matrix is equivalent to ordering the eigenvalues based on decomposing $\boldsymbol{\Sigma}_{XX}$ from largest to smallest, and then producing a matrix made up of the corresponding eigenvectors.

Let's go back to our example. 

```{r}
# Compute variance-covariance matrix of predictors
sigma_xx = cov(eeo[ , c("faculty", "peer")])

# Eigendecomposition
eigen(sigma_xx)

# Matrix of new basis vectors
basis = eigen(sigma_xx)$vectors

# Coordinates in original predictor space (row vector)
# These are often centered
old = t(c(0.608, 0.0351))

# Compute rotated values under the new basis
old %*% basis
```



<br />

## Using `princomp()` to Obtain the Principal Components

We can also use the R function `princomp()` to obtain the principal components based on the eigendecomposition. We provide this function with a data frame of the predictors. 

```{r}
# Select predictors
eeo_pred = eeo %>%
  select(faculty, peer)

# Create princomp object
my_pca = princomp(eeo_pred)

# View output
summary(my_pca, loadings = TRUE)
```
The values of the principal components are given in the `loadings` output. Note that the signs of the loadings are arbitrary. For example, the vector associated with PC1 could also have been $(-0.763, -0.647)$ and that for PC2 could have been $(-0.647, 0.763)$. The variances of each component can be computed by squaring the appropriate standard deviations in the output.

```{r}
# Compute variance of PC1
1.4002088 ^ 2

# Compute variance of PC2
0.19725327 ^ 2
```
Remember, the variances are the eigenvalues from the eigendecomposition. Also, since the principal components are orthogonal, we can sum the variances to obtain a total measure of variation in the original set of predictors accounted for by the principal components.

```{r}
# Compute total variation accounted for
total_var = 1.4002088 ^ 2 + 0.19725327 ^ 2
total_var

# Compute variation accounted for by PC1
(1.4002088 ^ 2) / total_var

# Compute variation accounted for by PC2
(0.19725327 ^ 2) / total_var
```
This suggests that PC1 accounts for 98\% of the variance in the original set of predictors and that PC2 accounts for 2\% of the variance. Note that these values are also given in the `summary()` output. We can also obtain the PC scores for each observation by accessing the `scores` element of the `princomp` object. (Below we only show the first 10 scores.) 

```{r eval=FALSE}
# Get PC scores
my_pca$scores
```

```{r echo=FALSE}
# Get PC scores
my_pca$scores[1:10, ]
```

These are slightly different than the scores we obtained by multiplying the original predictor values by the new basis matrix. For example, the PC scores for the first observation were $-0.486$ and $0.367$. The `princomp()` function mean centers each variable prior to multiplying by the basis matrix.

```{r}
# Mimic scores from princomp()
old = t(c(0.608 - mean(eeo$faculty), 0.0351 - mean(eeo$peer)))

# Compute PC scores
old %*% basis
```

<aside>
Because we want the prinicpal components to be soley functions of the predictors, we mean center them. Otherwise we would have to include a column of ones (intercept) in the **X** matrix. Mean centering the variables makes each variable orthogonal to the intercept, in which case it can be ignored. (This is simlar to how mean centering predictors removes the intercept from the fitted equation.)
</aside>

Since we only have two principal components, we can visualize the scores using a scatterplot.

```{r fig.width=6, fig.height=6, fig.cap="Rotated predictor space using the principal components as the new basis."}
# Create data frame of scores
pc = data.frame(my_pca$scores)

# Plot the scores
ggplot(data = pc, aes(x = Comp.1, y = Comp.2)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "lightgrey") +
  geom_vline(xintercept = 0, color = "lightgrey") +
  scale_x_continuous(name = "Principal Component 1", limits = c(-4, 4)) +
  scale_y_continuous(name = "Principal Component 2", limits = c(-4, 4)) +
  theme_bw() +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )
```

Conceptually this visualization depicts rotating the scatterplot showing the initial values to a new set of basis axes. The figure below, shows the rotated predictor space after re-orienting the rotated coordinate system. From this visualization, it is clear that there is much more variation in the values of PC1 than in PC2.

<br />

### Using All Three Predictors in the PCA

In the example we were focused on only two predictors, as it is pedagogically easier to conceptualize since the plotting is easier. However, PCA is directly extensible to more than two variables. With three variables, the data ellipse is a data ellipsoid and there are three principal components corresponding to the three orthogonal semi-axes of the ellipsoid. With four or more variables the ideas extend although the visual doesn't.

We will again use the `princomp()` function to compute the principal components and rotated scores. 

<aside>
The `cutoff=0` argument in the `summary()` function prints all the loadings. By default, only loadings above 0.1 are displayed.
</aside>

```{r}
# Select predictors
eeo_pred = eeo %>%
  select(faculty, peer, school)

# Create princomp object
my_pca = princomp(eeo_pred)

# View output
summary(my_pca, loadings = TRUE, cutoff = 0)
```

The rotation matrix, or loading matrix, is:

$$
\mathbf{W} =\begin{bmatrix}0.617 & 0.670 & 0.412 \\ 0.524 & -0.741 & 0.420 \\ 0.587 & -0.043 & -0.809\end{bmatrix}
$$

The first principal component again accounts for most of the variance in the three predictors (98.4%). The other two principal components account for an additional 1.3% and 0.3% of the variance in the predictor space.

Remember that each principal component is defining a composite variable composed of all three predictors. The loadings, which are the weights in computing the composite variables, can also be interpreted as the correlations between each particular variable and the composite. And, although the signs are arbitrary, we can try to interpret the differences in direction. So, for example,

- The composite variable formed by the first principal component is moderately and positively correlated with all three predictors. 
- The second composite variable is highly positively correlated with the faculty variable, highly negatively correlated with the peer variable, and not correlated with the school variable.
- The third composite variable is positively and moderately correlated with the faculty and peer variables, and highly negatively correlated with the school variable.

Sometimes these patterns of correlations can point toward an underlying latent factor, but this is a subjective call by the researcher based on their substantive knowledge. Other times the patterns make no sense; the results, after all, are just a mathematical result based on the variances and covariances of the variables.

Here, the first composite might be interpreted as an overall measurement of the three predictors since all the loadings are in the same direction and at least of moderate size. The second composite seems to represent a contrast between faculty credentials and peer influence due to the opposite signs on these loadings. While the third composite points toward a more complex contrast between school facilities and the combined faculty credentials/peer group influence.

<br />


### Using the Principal Components in a Regression Model

Remember, we undertook the PCA because of the collinearity in the original predictors. Rather than using the original predictor values in our regression, we can use the scores from the PCA. Since we created the principal components to be orthognal, this should alleviate any collinearity problems. 


```{r}
# Create data frame of PC scores
pc = data.frame(my_pca$scores)

# Add the PC scores to the original data
eeo2 = eeo %>%
  cbind(pc)

# View data
head(eeo2)

# Fit model using PC scores
lm.pc = lm(achievement ~ 1 + Comp.1 + Comp.2 + Comp.3, data = eeo2)

# Check for collinearity -- correlations
eeo2 %>%
  select(Comp.1, Comp.2, Comp.3) %>%
  correlate()

# Check for collinearity -- VIF
vif(lm.pc)
```

Examining some of the collinearity diagnostics we see that the predictors in this model are completely uncorrelated and the VIF values are 1; indicating that the SEs for these coefficients are exactly as large as they would be if the predictors were independent (which they are). Looking at the model- and coefficient-level output:

```{r}
# Model-level output
glance(lm.pc)

# Coefficient-level output
tidy(lm.pc, conf.int = 0.95)
```

The model-level output from this model is exactly the same as the model-level output from the model fitted with the original predictors. The coefficient-level output is where we start to see differences. Because there is no collinearity in this model, we now see a statistically significant result at the coefficient level (PC1). The other thing to remember here is that each principal component is a composite variable composed of all three predictors and perhaps had a more substantive interpretation. We need to use those substantive interpretations in the interpretations of coefficients:

- The intercept is the predicted average achievement for cases where all the composite variables are 0.
- The positive slope associated with the first composite indicates that higher values on this composite are associated with higher achievement, on average. In other words, higher values on all three predictors are associated with higher achievement, on average.
- The negative slope associated with the second composite indicates that higher values on this composite are associated with lower achievement, on average. In other words, larger contrasts between faculty credentials and peer influence are associated with lower achievement, on average.
- The positive slope associated with the third composite indicates that higher values on this composite are associated with higher achievement, on average. In other words, larger contrasts between school facilities and faculty credentials/peer influence are associated with higher achievement, on average.

Again, these interpretations may not be satisfactory---it all depends on whether the loadings offer a reasonable interpretation of the composite variable.

<br />


## Dimension Reduction

One of the most useful qualities of a PCA, aside from fixing collinearity issues, can be to reduce the size of the predictor space in a regression model; thereby reducing the complexity of the model and improving statistical power. To do this, we consider the variance accounted for by each of the principal components. 

In our example, the first principal component accounted for most of the variance in the original predictor space (approximately 98%). The other two principal components did not account for that much variation, which suggests that they may not be necessary. We can capture most of the variation in the original three predictors by just using the first principal component.

```{r}
# Fit reduced model using PC1
lm.pc.2 = lm(achievement ~ 1 + Comp.1, data = eeo2)

# Model-level output
glance(lm.pc.2)

# Coefficient-level output
tidy(lm.pc.2, conf.int = 0.95)
```
In this model, the model $R^2$ value is slightly smaller (0.183 rather than 0.206) since the first principal component did not account for all of the variance in the original predictors. However, this difference is negligible, and the model is still explains a statistically relevant amount of variation in achievement scores, $F(1,68)=15.25$, $p=0.0002$. 

From the coefficient-level output, we see that the magnitude of the intercept and slope are comparable to those in the model where all three composites were used. The standard error and *p*-value for the composite in this model are slightly smaller than when we used all of the predictors. This represents the additional two degrees of freedom in the error term that we got from reducing the number of predictors. (In this example, the differences are negligible.)

<br />


# SVD Decomposition

Another decomposition method that creates orthogonal unit vectors (i.e., basis) is SVD decomposition. Recall this decomposition method decomposes a matrix **A** as follows:

$$
\mathbf{A} = \mathbf{U}\mathbf{D}\mathbf{V}^{\intercal}
$$

where, **U** and **V** are an orthogonal matrices and **D** is a diagonal matrix. In PCA, we are interested in performing SVD on the covariance matrix $\mathbf{X}^{\intercal}\mathbf{X}$. To carry out the SVD decomposition, we use the `svd()` function. Below, we create a matrix of the three predictors and then carry out the SVD decomposition on the covariance matrix.


```{r}
# Create matrix of predictors
X = as.matrix(eeo[ , c("faculty", "peer", "school")])

# SVD decomposition
sv_decomp = svd(t(X) %*% X)

# View results
sv_decomp
```



$$
\begin{split}
\mathbf{X}^{\intercal}\mathbf{X} &= \mathbf{U}\mathbf{D}\mathbf{V}^{\intercal} \\[1em]
\begin{bmatrix}81.123 & 66.518 & 75.512 \\ 66.518 & 59.163 & 64.251 \\ 75.512 & 64.251 & 72.358\end{bmatrix} &= \begin{bmatrix}0.617 & 0.670 & -0.412 \\ -0.524 & -0.741 & -0.419 \\ -0.586 & -0.042 & 0.809\end{bmatrix} \begin{bmatrix}209.330 & 0 & 0 \\ 0 & 2.730 & 0 \\ 0 & 0 & 0.583 \end{bmatrix} \begin{bmatrix}-0.617 & -0.524 & -0.586 \\ 0.670 & -0.741 & -0.042 \\ -0.412 & -0.419 &  0.809 \end{bmatrix}
\end{split}
$$

Mathematically, since any matrix can be decomposed using SVD, we can also decompose $\mathbf{X} = \mathbf{U}\mathbf{D}\mathbf{V}^{\intercal}$. Then we can write $\mathbf{X}^{\intercal}\mathbf{X}$ as:

$$
\mathbf{X}^{\intercal}\mathbf{X} = (\mathbf{U}\mathbf{D}\mathbf{V}^{\intercal})^{\intercal} (\mathbf{U}\mathbf{D}\mathbf{V}^{\intercal})
$$
Re-expressing this we get:

$$
\begin{split}
\mathbf{X}^{\intercal}\mathbf{X} &= \mathbf{V}\mathbf{D}^{\intercal}\mathbf{U}^{\intercal}\mathbf{U}\mathbf{D}\mathbf{V}^{\intercal} \\[0.5em]
\end{split}
$$

Since **D** is a diagonal matrix, $\mathbf{D}^{\intercal}\mathbf{D} = \mathbf{D}^2$, so reducing this expression gives

$$
\begin{split}
\mathbf{X}^{\intercal}\mathbf{X} &= \mathbf{V}\mathbf{D}^2\mathbf{V}^{\intercal} \\[0.5em]
\end{split}
$$
The matrices **V** and $\mathbf{V}^{\intercal}$ are both orthogonal basis matrices that ultimately act to rotate the Cartesian bases. The $\mathbf{D}^2$ matrix is diagonalizing the covariance matrix which amounts to finding the the major axes in the data ellipse along which our data varies.



## Using the SVD Decomposition as PCA

From a dimension reduction/PCA perspective, what is interesting is the **V** matrix.

$$
\mathbf{V} = \begin{bmatrix}-0.617 & 0.670 & -0.412 \\ -0.524 &  -0.741 & -0.419 \\ -0.586 & -0.042  & 0.809 \end{bmatrix}
$$

These are the principal components. The values in the **D** matrix are related to the eigenvalues, and thus the variances of the principal components.

$$
\lambda = \frac{d_{ii} ^ 2}{n-1}
$$

Squaring each of the diagonal elements of **D** and dividing by the total degrees of freedom gives us the associated eigenvalues. Then we can use these to compute the proportion of variance for each of the principal components.

```{r}
# Compute lambda values (variances)
lambda = sv_decomp$d ^ 2 / (70 - 1)
lambda


# Compute proportion of variance
lambda / sum(lambda)
```

<aside>
If the SVD is carried out on the correlation matrix of the predictors rather than the variance--covariance matrix, the values in **D** are the $\lambda$ values. Note also that the `cov()` function in R produced a variance--covariance matrix that has already been divided by $n-1$, so if that is the input to the SVD, then there is also no need to divide by $n-1$ in the 
</aside>

The principal component scores can be obtained by postmultiplying the mean centered predictor matrix **X** by the **V** matrix.

```{r}
# Mean center each predictor
X[, 1] = X[, 1] - mean(X[, 1])
X[, 2] = X[, 2] - mean(X[, 2])
X[, 3] = X[, 3] - mean(X[, 3])

# Compute PC scores
pc_scores = X %*% sv_decomp$v
head(pc_scores)
```

The `prcomp()` function carries out PCS using SVD decomposition. Different elements of the The `prcomp()` object can then be accessed to print output that is important to a PCA.

```{r}
# PCA using SVD decomposition
my_pca2 = prcomp(X)

# Get standard deviations/proportion of variance
summary(my_pca2)

# Get matrix of principal components (eigenvector matrix)
my_pca2$rotation

# Get PC scores
my_pca2$x
```

<aside>
In general it is more efficient to use singular value decomposition than eigendecomposition when carrying out a PCA. As such the use of `prcomp()` is recommended. 
</aside>


<br />

# PCA from the Correlation Matrix

So far we have been applying the PCA to the covariance matrix. It can often make more sense to apply PCA to the correlation matrix, especially when the variables are measured in different metrics or have varying degrees of magnitude. In these cases, using the covariance matrix will often result in results in which variables with large magnitudes of scale dominate the PCA. To use the correlation matrix rather than the covariance matrix when computing the prinicpal components, we use the argument `scale = TRUE` in the `prcomp()` function.

```{r}
# PCA using SVD decomposition; use correlation matrix of predictors
my_pca3 = prcomp(X, scale = TRUE)

# Get standard deviations/proportion of variance
summary(my_pca3)

# Get matrix of principal components (eigenvector matrix)
my_pca3$rotation

# Get PC scores
my_pca3$x
```

The variance accounted for by the principal components is comparable; the scaling does not change this. The actual principal components are different because of the scaling, but the interpretations are the same as when we used the covariance matrix. Similarly, because of the different scaling, the PC scores are different.

<aside>
Here the results from using the correlation matrix are not that different as the variables were already *z*-scores.
</aside>


Typically applied researchers will use the correlation matrix rather than the covariance matrix to perform a PCA. Working on the correlation matrix is akin to working with standardized variables. (In fact the correlation matrix is simply the variance--covariance matrix of the standardized predictors.) This protects the PCA from being dominated by variables with numerically large scales. It also is helpful when the predictors are measured using qualitatively different scales (e.g., one is measured in dollars and another in years of education).



<!-- ```{r fig.width=6, fig.height=6, out.width='50%'} -->
<!-- X = model.matrix(lm.1)[ , -c(1,4)] -->

<!-- rot = eigen(crossprod(X))$vectors -->
<!-- new_X = data.frame(X %*% -rot) -->
<!-- names(new_X) = c("faculty", "peer") -->

<!-- eeo2 = rbind(eeo[ , c("faculty", "peer")], new_X) %>% -->
<!--   mutate(data_set = c(rep("Orignal", 70), rep("Rotated", 70))) -->

<!-- library(gganimate) -->


<!-- ggplot(data = eeo, aes(x = faculty, y = peer)) + -->
<!--   geom_point() + -->
<!--   geom_segment(x = 0, xend = 1, y = 0, yend = 0, arrow = arrow(length = unit(0.1,"cm"))) + -->
<!--   geom_segment(x = 0, xend = 0, y = 0, yend = 1, arrow = arrow(length = unit(0.1,"cm"))) + -->
<!--   scale_x_continuous(name = "Standardized measure of faculty's credentials", limits = c(-3, 3)) + -->
<!--   scale_y_continuous(name = "Standardized measure of peer group influence", limits = c(-3, 3)) + -->
<!--   theme_bw() + -->
<!--   geom_segment(x = 0, xend = -0.7625172, y = 0, yend = -0.6469680, arrow = arrow(length = unit(0.1,"cm")), col = "red") + -->
<!--   geom_segment(x = 0, xend = 0.6469680, y = 0, yend = -0.7625172, arrow = arrow(length = unit(0.1,"cm")), col = "red") -->

<!-- # Rotated space -->
<!-- ggplot(data = eeo2, aes(x = faculty, y = peer)) + -->
<!--   geom_point() + -->
<!--   geom_segment(x = 0, xend = 1, y = 0, yend = 0, arrow = arrow(length = unit(0.1,"cm"))) + -->
<!--   geom_segment(x = 0, xend = 0, y = 0, yend = 1, arrow = arrow(length = unit(0.1,"cm"))) + -->
<!--   scale_x_continuous(name = "Standardized measure of faculty's credentials", limits = c(-4, 4)) + -->
<!--   scale_y_continuous(name = "Standardized measure of peer group influence", limits = c(-4, 4)) + -->
<!--   theme_bw() -->



<!-- ggplot(data = eeo2, aes(x = faculty, y = peer)) + -->
<!--   geom_point() + -->
<!--   #geom_segment(x = 0, xend = 1, y = 0, yend = 0, arrow = arrow(length = unit(0.1,"cm"))) + -->
<!--   #geom_segment(x = 0, xend = 0, y = 0, yend = 1, arrow = arrow(length = unit(0.1,"cm"))) + -->
<!--   scale_x_continuous(name = "Standardized measure of faculty's credentials", limits = c(-4, 4)) + -->
<!--   scale_y_continuous(name = "Standardized measure of peer group influence", limits = c(-4, 4)) + -->
<!--   theme_bw() + -->
<!--   transition_states(data_set, -->
<!--                     transition_length = 2, -->
<!--                     state_length = 1) -->

<!-- ``` -->



<br />





# References
