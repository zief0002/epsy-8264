---
title: "Cross-Validation"
author: "Andrew Zieffler"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: 
  html_document:
    highlight: zenburn
    css: ['style/style.css', 'style/table-styles.css', 'style/syntax.css', 'style/notes.css']
bibliography: epsy8264.bib
csl: 'style/apa-single-spaced.csl'
---

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


```{r setup, include=FALSE}
library(knitr)
library(kableExtra)
library(patchwork)

knitr::opts_chunk$set(
  echo = TRUE, 
  fig.width = 6, 
  fig.height = 6, 
  fig.align = "center",
  out.width = "40%", 
  message = FALSE, 
  warning = FALSE
  )
```

In the previous set of notes, we examined predictors of average life expectancy in a state, using data from *states-2019.csv*. We used several model building strategies and performance metrics to select and evaluate predictors. Unfortunately, we were evaluating the model/predictors using the same data that we used to build the model. This has several problems in practice, especially if you are using inferential metrics (*p*-values) to select the predictors: 

- **Increased Type I error rates:** If you use *p*-values to select predictors, there are many tests you are examining across the model selection process. You probably need to adjust the $p$-values obtained from these tests to accommodate this. 
- **Overfit/Lack of generalization:** The adopted model still has the potential to not perform well on future data.

<br />

## Preparation



```{r message=FALSE}
# Load libraries
library(car)
library(corrr)
library(modelr)
library(patchwork)
library(tidyverse)
library(tidymodels) # Loads broom, rsample, parsnip, recipes, workflow, tune, yardstick, and dials

# Import and view data
usa = read_csv("../data/states-2019.csv")
head(usa)

# Create standardized variables after removing state names
z_usa = scale(usa[ , -1]) %>%
  data.frame()
```

From our previous notes, there were a several candidate models that we may have been considering based on their AICc values. In this set of notes we will consider the best *k*-predictor models that had an AICc value within four of the minimum AICc value. These models include the following predictors:

- income
- income, population
- income, population, illiteracy rate
- income, population, illiteracy rate, murder rate

<br />

## Evaluating Predictors: Cross-Validation

A better method for model selection, that does not use testing, is *cross-validation*. To perform cross-validation, you randomly split the data set into two parts: a *training* set, and a *validation* set. Any candidate models are then fitted on the training data and evaluated using the validation data. Because we use a different data set to evaluate the models, the resulting evaluation is not biased toward the data we used to fit the model (i.e., less overfit). Because of this, we get better indications about the generalizability of the models to new/future data.

In our regression example, the algorithm for performing the cross-validation is:

1. Randomly divide the `z_usa` data into two sets of observations; training and validation data sets.
2. Fit the four candidate models to the training observations. 
3. Use the estimated coefficients from those fitted models to estimate the fitted values and residuals for the observations in the validation data. 
4. Based on the residuals from the validation observations, compute measures of model performance (e.g., MSE).

Next, we will explore and carry out each of the steps in this algorithm to adopt a model for the `z_usa` data.


<br />

### Divide the Data into a Training and Validation Set

There are many ways to do this, but I will use the `sample()` function to randomly sample 35 case numbers (about 2/3 of the data), from 1 to 52 (the number of rows in the `z_usa` data), to make up the training data. Then I use the `filter()` function to select those cases. The remaining cases (those 17 observations not sampled) are put into a validation set.

```{r}
# Make the random sampling replicable
set.seed(42)

# Select the cases to be in the training set
training_cases = sample(1:nrow(z_usa), size = 35, replace = FALSE)

# Create training data
train = z_usa %>%
  filter(row_number() %in% training_cases)

# Create validation data
validate = z_usa %>%
  filter(!row_number() %in% training_cases)
```


<br />

### Fit the Candidate Models to the Training Data

We now fit the four candidate models to the observations in the training data.

```{r}
lm.1 = lm(life_expectancy ~ -1 + income,                                    data = train)
lm.2 = lm(life_expectancy ~ -1 + income + population,                       data = train)
lm.3 = lm(life_expectancy ~ -1 + income + population + illiteracy,          data = train)
lm.4 = lm(life_expectancy ~ -1 + income + population + illiteracy + murder, data = train)
```

<br />

### Fit the Models Obtained to the Validation Observations

Now we can obtain the predicted life expectancies for the observations in the validation data based on the coefficients from the models fitted to the training data.

```{r}
# Get the predicted values for the validation data
yhat_1 = predict(lm.1, newdata = validate)
yhat_2 = predict(lm.2, newdata = validate)
yhat_3 = predict(lm.3, newdata = validate)
yhat_4 = predict(lm.4, newdata = validate)
```

These vectors of fitted values can be used to compute the residuals for the validation observations, which in turn can be used to compute the *Cross-Validated Mean Square Error* (CV-MSE), a measure of out-of-sample performance. The CV-MSE is computed as:

$$
\mathrm{CV\mbox{-}MSE} = \frac{1}{n} \sum_{i=1}^n e_i^2
$$

where $n$ is the number of observations in the validation set. We can compute the CV-MSE for each of the candidate models.

```{r}
sum((validate$life_expectancy - yhat_1) ^ 2) / nrow(validate)
sum((validate$life_expectancy - yhat_2) ^ 2) / nrow(validate)
sum((validate$life_expectancy - yhat_3) ^ 2) / nrow(validate)
sum((validate$life_expectancy - yhat_4) ^ 2) / nrow(validate)
```

The candidate model having the smallest CV-MSE is the model that should be adopted. In this case, the values for the CV-MSE suggest adopting the single predictor model.


<br />

## Leave-One-Out Cross-Validation

There are two major problems with the cross-validation we just performed.

1. The estimate of the CV-MSE is highly dependent on the observations chosen to be in the training and validation sets. This is illustrated in the figure below which shows the CV-MSE values for the four models for 10 different random splits of the data. Some of these splits support adopting the quadratic  polynomial model, other splits support adopting the cubic polynomial model, and one supports the linear model!


```{r fig.width = 8, fig.height = 4, echo=FALSE, warning=FALSE, fig.cap="Model that has the lowest CV-MSE for 10 random splits of the data."}
my_list = list()

set.seed(42)

my_seeds = sample(1:500, size = 9)

choose_seed = c(42, my_seeds)

for(i in 1:10){

  set.seed(choose_seed[i])

  # Select the cases to be in the training set
  training_cases = sample(1:nrow(z_usa), size = 35, replace = FALSE)

  # Create training data
  train = z_usa %>%
    filter(row_number() %in% training_cases)

  # Create validation data
  validate = z_usa %>%
    filter(!row_number() %in% training_cases)

  lm.1 = lm(life_expectancy ~ -1 + income,                                    data = train)
  lm.2 = lm(life_expectancy ~ -1 + income + population,                       data = train)
  lm.3 = lm(life_expectancy ~ -1 + income + population + illiteracy,          data = train)
  lm.4 = lm(life_expectancy ~ -1 + income + population + illiteracy + murder, data = train)

  yhat_1 = predict(lm.1, newdata = validate)
  yhat_2 = predict(lm.2, newdata = validate)
  yhat_3 = predict(lm.3, newdata = validate)
  yhat_4 = predict(lm.4, newdata = validate)

  my_list[[i]] = data.frame(
    k = c(1, 2, 3, 4),
    cv_mse = c(
      sum((validate$life_expectancy - yhat_1)^2) / nrow(validate),
      sum((validate$life_expectancy - yhat_2)^2) / nrow(validate),
      sum((validate$life_expectancy - yhat_3)^2) / nrow(validate),
      sum((validate$life_expectancy - yhat_4)^2) / nrow(validate)
      ),
    data_set = i
  )
}

my_results = do.call(rbind, my_list)

# ggplot(data = my_results, aes(x = k, y = cv_mse, group = factor(data_set), color = factor(data_set))) +
#   geom_line() +
#   theme_bw() +
#   scale_x_continuous(name = "Best k-Predictor Model", breaks = 1:4) +
#   scale_y_continuous(name = "CV-MSE", limits = c(0, 3.5)) +
#   ggsci::scale_color_d3(name = "") +
#   guides(color = FALSE)

my_results %>% group_by(data_set) %>% filter(cv_mse == min(cv_mse)) %>% ungroup() %>%
ggplot(aes(x = k)) +
  geom_dotplot() +
  theme_bw() +
  scale_x_continuous(name = "Best k-Predictor Model", breaks = 1:4) +
  scale_y_continuous(name = "", breaks = NULL)
```

2. Only a subset of the observations (those in the training set) are used to initially fit the model. Since statistical methods tend to perform worse when trained on fewer observations, this suggests that the CV-MSE may tend to overestimate the error rate (model accuracy measure) for the model fit.

Leave-one-out cross-validation (LOOCV) is one method that can be used to overcome these issues. The algorithm for performing LOOCV is:

1. Hold out the $i$th observation as your validation data (a single observation) and use the remaining $n-1$ observations as your training data.
3. Fit all candidate models to the training data.
4. Use the estimated coefficients from those fits to compute $\mathrm{MSE}_i$ using the validation data.
5. Repeat Steps 2--4 for each observation.

We then have $n$ estimates of the MSE that can be averaged to get the overall CV-MSE.

$$
\mathrm{CV\mbox{-}MSE} = \frac{1}{n} \sum_{i=1}^n \mathrm{MSE}_i
$$


Since the validation dataset is composed of a single observation ($n=1$), $\mathrm{MSE}_i$ is simply the squared residual value for the validation observation.

$$
\begin{split}
\mathrm{MSE} = \frac{1}{n} \sum_{i=1}^n e_i^2 \\[0.5em]
= \frac{1}{1} \sum_{i=1}^n e_i^2 \\[0.5em]
= \sum_{i=1}^n e_i^2
\end{split}
$$



We can carry out LOOCV by using a `for()` loop to iterate through the algorithm for each of the $n$ observations. Here is some example syntax:

```{r}
# Set up empty vector to store results
mse_1 = rep(NA, 52)
mse_2 = rep(NA, 52)
mse_3 = rep(NA, 52)
mse_4 = rep(NA, 52)
mse_5 = rep(NA, 52)

# Loop through the cross-validation
for(i in 1:nrow(z_usa)){
  train = z_usa %>% filter(row_number() != i)
  validate = z_usa %>% filter(row_number() == i)

  lm.1 = lm(life_expectancy ~ -1 + income,                                    data = train)
  lm.2 = lm(life_expectancy ~ -1 + income + population,                       data = train)
  lm.3 = lm(life_expectancy ~ -1 + income + population + illiteracy,          data = train)
  lm.4 = lm(life_expectancy ~ -1 + income + population + illiteracy + murder, data = train)

  yhat_1 = predict(lm.1, newdata = validate)
  yhat_2 = predict(lm.2, newdata = validate)
  yhat_3 = predict(lm.3, newdata = validate)
  yhat_4 = predict(lm.4, newdata = validate)

  mse_1[i] = (validate$life_expectancy - yhat_1) ^ 2
  mse_2[i] = (validate$life_expectancy - yhat_2) ^ 2
  mse_3[i] = (validate$life_expectancy - yhat_3) ^ 2
  mse_4[i] = (validate$life_expectancy - yhat_4) ^ 2

}

# Compute CV-MSE
mean(mse_1)
mean(mse_2)
mean(mse_3)
mean(mse_4)
```

The LOOCV results point toward the three-predictor model (smallest average CV-MSE), although the CV-MSE for the two-predictor model is not that much larger. Since this method is less biased; it trains the models on a much larger set of observations, its results are more believable than the simple cross-validation. Another advantage of LOOCV is that it will always produce the same results as opposed to simple cross-validation) since there is no randomness in producing the training and validation datasets.

LOOCV can be computationally expensive; we have to fit the candidate models $n$ times. It turns out, however, that models fit with least squares linear or polynomial regression, can give us the LOOCV results using the following formula:

$$
\mathrm{CV\mbox{-}MSE} = \frac{1}{n} \sum_{i=1}^n \bigg(\frac{e_i}{1-h_{ii}}\bigg)^2
$$

where $\hat{y}_i$ is the $i$th fitted value from the original least squares fit, and $h_{ii}$ is the leverage value. This is similar to the model (biased) MSE, except the $i$th residual is divided by $1 − h_{ii}$.


<!-- ```{r} -->
<!-- # Compute CV-MSE for best three-predictor model -->
<!-- lm.3 = lm(life_expectancy ~ -1 + income + population + illiteracy, data = z_usa) -->

<!-- # Augment the model to get e_i and h_ii -->
<!-- out.3 = augment(lm.3) -->

<!-- # Compute CV-MSE for best three-predictor model -->
<!-- 1 / 52 * sum((out.3$.resid / (1 - out.3$.hat))^2) -->
<!-- ``` -->

LOOCV is a very general method, and can be used with any kind of modeling. For example we could use it with logistic regression, or mixed-effects analysis, or any of the methods you have encountered in your statistics courses to date. That being said, the formula does not hold for all these methods, so outside of linear and polynomial regression, the candidate models will actually need to be fitted $n$ times.

<br />


## k-Fold Cross-Validation

One alternative to LOOCV is $k$-fold cross-validation. The algorithm for $k$-fold cross-validation is:

1. Randomly divide the data into $k$ groups or folds.
2. Hold out the $i$th fold as your validation data and use the remaining $k-1$ folds as your training data.
3. Fit all candidate models to the training data.
4. Use the estimated coefficients from those fits to compute $\mathrm{MSE}_i$ using the validation data.
5. Repeat Steps 2--4 for each fold.

We then have $k$ estimates of the MSE that can be averaged to get the overall CV-MSE.

$$
\mathrm{CV\mbox{-}MSE} = \frac{1}{k} \sum_{i=1}^k \mathrm{MSE}_i
$$

From this algorithm, it is clear that LOOCV is a special case of $k$-fold cross-validation in which $k$ is set to equal $n$. In practice we typically use $k=5$ or $k=10$.


We can carry out $k$-fold cross-validation by using the `crossv_kfold()` function from the **modelr** package. This function takes the argument `k=` to indicate the number of folds. Here is some example syntax to carry out a 10-fold cross-validation:

```{r}
# Divide data into 10 folds
set.seed(100)

my_cv = z_usa %>%
  crossv_kfold(k = 10)
```

Then we will use the `map()` and `map2_dbl()` functions from the **purrr** package to fit a model to the training (`train`) data and find the MSE on the validation (`test`) data created from the `crossv_kfold()` function. We will have to carry this out for each of the candidate models.

:::fyi
For more detailed information about using the purrr functions, see Jenny Bryan's fantastic [purrr tutorial](https://jennybc.github.io/purrr-tutorial/index.html).
:::

```{r}
# Best 1-predictor model
cv_1 = my_cv %>%
  mutate(
    model = map(train, ~lm(life_expectancy ~ 1 + income, data = .)),
    MSE = map2_dbl(model, test, modelr::mse),
    k = 1
    )

# Best 2-predictor model
cv_2 = my_cv %>%
  mutate(
    model = map(train, ~lm(life_expectancy ~ 1 + income + population, data = .)),
    MSE = map2_dbl(model, test, modelr::mse),
    k = 2
    )

# Best 3-predictor model
cv_3 = my_cv %>%
  mutate(
    model = map(train, ~lm(life_expectancy ~ 1 + income + population + illiteracy, data = .)),
    MSE = map2_dbl(model, test, modelr::mse),
    k = 3
    )

# Best 4-predictor model
cv_4 = my_cv %>%
 mutate(
    model = map(train, ~lm(life_expectancy ~ 1 + income + population + illiteracy + murder, data = .)),
    MSE = map2_dbl(model, test, modelr::mse),
    k = 4
    )
```

Once we have the results, we can stack these into  single data frame and then use `group_by()` and `summarize()` to obtain the CV-MSE estimates.

```{r}
rbind(cv_1, cv_2, cv_3, cv_4) %>%
  group_by(k) %>%
  summarize(
    cv_mse = mean(MSE)
  )
```


The results of carrying out the 10-fold cross-validation suggest that we adopt the best two- or three-predictor model.

Using $k$-fold cross-validation is computationally less expensive so long as $k<n$. But this computational gain has a cost in that the results are again dependent on the $k$ random splits. This variation, however, is less than that in the single split simple cross-validation. We can also alleviate some of this by fitting the $k$-fold cross-validation several times and averaging across the results to get the CV-MSE estimate.

:::fyi
There are several R packages that will fit a $k$-fold cross-validation (e.g., **DAAG**).
:::

<br />

## Model Selection

The different methods of selecting variables suggest we adopt the following models:

```{r echo=FALSE}
tab_01 = data.frame(
  Method = c("FS (t-value)", "FS (AIC)", "BE (R2)", "All Subsets (AICc)", "Simple CV", "Leave-One-Out CV", "10-Fold CV"),
  Model = c("Income, Population, Illiteracy, Murder", "Income, Population, Illiteracy", "Income, Population, Illiteracy", "Income, Population, Illiteracy", "Income", "Income, Population, Illiteracy", "Income, Population, Illiteracy")
)

kable(
  tab_01,
  caption = "Predictors selected from forward selection (FS), backward elimination (BE) and cross-validation (CV) methods. The CV-MSE was used as a performance metric for model selection in all cross-validation methods.",
  table.attr = "style='width:60%;'"
  )  %>%
  kable_styling() %>%
  row_spec(row = 0, align = "c")
  
```

:::fyi
While the different CV methods can suggest different models, this is usually less problematic when the sample size is larger; we only have 52 observations in the `z_usa` data.
:::

The LOOCV and $k$-fold CV are better suited for considering how the model will perform on future data sets, so I would adopt the model that includes income, population, and illiteracy rate. To get the coefficient estimates, we fit whichever model we adopt to the FULL data set (with all the observations) as this will give us the "best" estimates.

```{r}
# Fit model
lm.3 = lm(life_expectancy ~ -1 + income + population + illiteracy, data = z_usa)

# Get model-level output
glance(lm.3)

# Get coefficient-level output
tidy(lm.3)
```


## Reporting Results from a Cross-Validation

When we report the results of a regression model evaluated using cross-validation, there are some subtle differences in what is reported. At the model-level, we report the $R^2$ from the adopted model fitted to the full-data. We also report the CV-MSE as a measure of the model error for future observations (generalized error).

At the coefficient-level we typically report the coefficient estimates and standard errors based on fitting the adopted model to the full data. However, we DO NOT REPORT NOR INTERPRET P-VALUES. The $p$-values do not take into account that the model was selected using cross-validation. Because of this they are incredibly misleading. As with any model, interpretations should also be offered, again typically by way of providing a plot of the fitted model to help facilitate these interpretations. For example:


The regression model using income, population, and illiteracy rate to predict variation in life expectancy was selected using 10-fold cross-validation (CV-MSE = 0.830). The model explains 34.9\% of the variation in life expectancies. The fitted equation is:

$$
\hat{\mathrm{Life~Expectancy}}_i = 0.488(\mathrm{Income}_i) + 0.392(\mathrm{Population}_i) - 0.310(\mathrm{Illiteracy~Raten}_i)
$$
where, the outcome and all predictors are standardized. This model suggests moderate, positive, partial effects of both income and population and a moderate negative effect of illiteracy rate on life expectancy.

<br />

## Information Criteria and Cross-Validation

In EPsy 8252, we learned about using information criteria for model selection. In particular, we used AIC, BIC, and AICc for model selection. It turns out the AIC-based model selection and cross-validation are asymptotically equivalent @Stone:1977. For linear models, using BIC for model selection is symptotically equivalent to leave-$v$-out cross-validation when $v = n\bigg(1 - \frac{1}{\ln (n) - 1}\bigg)$ [@Shao:1997].

As a practical note, computer simulations have suggested that the results from using AICc for model selection will, on average, be quite similar to those from cross-validation techniques. As such, AICc can be a useful alternative to the computationally expensive cross-validation methods. However, using AICc does not provide a measure of the model performance (MSE) in new datasets like cross-validation does.

```{r message=FALSE}
# Fit models to all data
lm.1 = lm(life_expectancy ~ -1 + income,                                    data = z_usa)
lm.2 = lm(life_expectancy ~ -1 + income + population,                       data = z_usa)
lm.3 = lm(life_expectancy ~ -1 + income + population + illiteracy,          data = z_usa)
lm.4 = lm(life_expectancy ~ -1 + income + population + illiteracy + murder, data = z_usa)


# Load library
library(AICcmodavg)

# Get AICc for all models
aictab(
  cand.set = list(lm.1, lm.2, lm.3, lm.4),
  modnames = c("Best 1-Predictor", "Best 2-Predictor", "Best 3-Predictor", "Best 4-Predictor")
)
```

The model with the lowest AICc is, indeed, the three-predictor model. There is also some evidence for the four-predictor model given the AICc weight.

<br />


## References



