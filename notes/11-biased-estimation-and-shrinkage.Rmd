---
title: "Biased Estimation: Ridge Regression"
description: |
  A brief introduction to ridge regression for dealing with collinearity.
author:
  - name: Andrew Zieffler 
    url: http://www.datadreaming.org/
date: "`r Sys.Date()`"
output:
  distill::distill_article:
    highlight: tango
bibliography: [epsy8264.bib]
csl: 'style/apa-single-spaced.csl'
---

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 6)
library(knitr)
library(kableExtra)
library(patchwork)
```

The data in *equal-educational-opportunity.csv* consist of data taken from a random sample of 70 schools in 1965 [@Coleman:1966; @Chatterjee:2012; @Mosteller:1972]. We will use these data to mimic one of the original regression analyses performed; examining whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credentials and peer influence.

```{r}
# Load libraries
library(broom)
library(car)
library(corrr)
library(glmnet)
library(tidyverse)


# Read in data
eeo = read_csv("../data/equal-education-opportunity.csv")
head(eeo)
```

To examine the RQ, the following model was posited:

$$
\mathrm{Achievement}_i = \beta_0 + \beta_1(\mathrm{Faculty}_i) + \beta_2(\mathrm{Peer}_i) + \beta_3(\mathrm{School}_i) + \epsilon_i
$$

Unfortunately, the model suffered from strong collinearity resulting in poor estimates of the coefficients and very large SEs. This means we cannot effectively control for peer influence and faculty credentials, which means we cannot answer the RQ. 

<br /> 

# Biased Estimation

The [Gauss-Markov Theorem](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem) posits many attractive features of the OLS regression model. Two of these properties are that the OLS estimators will be unbiased and that the sampling variances of the coefficients are as small as possible^[At least within the class of linear, unbiased estimators.]. In our example, the fitted model showed strong evidence of collinearity; which makes the SEs super large even if they are the minimum of all possible linear, unbiased estimates.  

One method of dealing with collinearity is to use a biased estimation method. These methods forfeit unbiasedness to decrease the size of the sampling variances; it is bias--variance tradeoff. The goal with these methods is to trade a small amount of bias in the estimate for a large reduction in the sampling variances for the coefficients.

<aside>
The bias--variance tradeoff is a commonplace, especially in prediction models. You can explore it in more detail at [http://scott.fortmann-roe.com/docs/BiasVariance.html](http://scott.fortmann-roe.com/docs/BiasVariance.html).
</aside>


# Ridge Regression

To date, the most commonly used biased estimation method in the social sciences is ridge regression. Instead of finding the coefficients that minimize the sum of squared errors, ridge regression finds the coefficients that minimize a penalized sum of squares, namely:

$$
\mathrm{SSE}_{\mathrm{Penalized}} = \sum_{i=1}^n \bigg(y_i - \hat y_i\bigg)^2 + d \sum_{j=1}^p \beta_j^2
$$

where $d\geq0$ is a scalar value, $\beta_j$ is the *j*th regression coefficient, and $y_i$ and $\hat{y}_i$ are the observed and model fitted values, respectively. One way to think about this formula is:

$$
\mathrm{SSE}_{\mathrm{Penalized}} = \mathrm{SSE} + \mathrm{Penalty}
$$

Minimizing over this penalized sum of squared error has the effect of "shrinking" the mean square error and coefficient estimates toward zero. As such, ridge regression is part of a larger class of methods known as *shrinkage methods*. 

The *d* value in the penalty term controls the amount of shrinkage. When $d = 0$, the entire penalty term is 0, and the $\mathrm{SSE}_{\mathrm{Penalized}} = \mathrm{SSE}$. Minimizing this will, of course, produce the OLS estimates. The bigger *d* is, the more the MSE and coefficients shrink toward zero. At the extreme end, $d = \infty$ will shrink every coefficients to zero.


<br />

## Matrix Formulation of Ridge Regression

Recall that the OLS estimates are given by:

$$
\boldsymbol{\beta} = (\mathbf{X}^{\intercal}\mathbf{X})^{-1}\mathbf{X}^{\intercal}\mathbf{Y}
$$
If the $\mathbf{X}^{\intercal}\mathbf{X}$ matrix is ill-conditioned, computing the inverse results in a fair amount of inaccuracy which translates into bad estimates of the coefficients and standard errors. Unfortunately collinearity causes the $\mathbf{X}^{\intercal}\mathbf{X}$ matrix to be ill-conditioned. By inflating the diagonal elements of $\mathbf{X}^{\intercal}\mathbf{X}$, we can condition the matrix, which will hopefully lead to more stable coefficient estimates.

<aside>
If a matrix has a high condition number as computed by $\kappa = \frac{\vert\lambda_{\mathrm{Max}}\vert}{\vert\lambda_{\mathrm{Min}}\vert}$, it is said to be ill-conditioned. 
</aside>



To do this, we can add some constant amount (*d*) to each of the diagonal elements of $\mathbf{X}^{\intercal}\mathbf{X}$, prior to finding the inverse, which, we can express as:

$$
\widetilde{\boldsymbol{\beta}} = (\mathbf{X}^{\intercal}\mathbf{X} + d \mathbf{I})^{-1}\mathbf{X}^{\intercal}\mathbf{Y}
$$
<aside>
Technically this equation is for standardized variables.
</aside>


## Ridge Regression in Practice: An Example

Let's fit a ridge regression model to our EEO data. For now, we will choose the value for *d*. Let's use $d=0.1$. Prior to any matrix calculations, we first standardize all potential variables in the model. (It is recommended that you always standardize all variables prior to fitting a ridge regression because large coefficients will impact the penalty in the SSE more than small coefficients.)

```{r}
# Standardize all variables in the eeo data frame
z_eeo = scale(eeo)

# View
head(z_eeo)
```

The design matrix, **X**, is a $70\times 3$ matrix (since the variables are standardized, the intercept in any regression will be set to zero and is not estimated). Here we use the `data.matrix()` function to create a design matrix based on the standardized predictors.

```{r}
# Create and view the design matrix
X = z_eeo[ , c("faculty", "peer", "school")]
head(X)
```

The **Y** matrix is a $70\times 1$ column matrix of the standardized outcome values, namely:

```{r}
# Create and view the outcome vector
Y = z_eeo[ , "achievement"]
head(Y)
```


Since $\mathbf{X}^{\intercal}\mathbf{X}$ is a $3\times 3$ matrix, $d \mathbf{I}$ must also be a $3\times 3$ matrix (to be able to sum them). Here we use $\lambda=0.1$ just to illustrate the concept of ridge regression:

```{r}
# Compute and view dI
dI = 0.1 * diag(3)
dI
```

Finally we can perform the matrix algebra to obtain the ridge regression coefficients:

```{r}
# Compute ridge regression coefficients
b = solve(t(X) %*% X + dI) %*% t(X) %*% Y
b
```

The fitted ridge regression model is then,

$$
\hat{\mathrm{Achievement}}^{\star}_i = 0.435(\mathrm{Faculty}^{\star}_i) + 0.855(\mathrm{Peer}^{\star}_i) - 0.849(\mathrm{School}^{\star}_i)
$$

where the star-superscript denotes a standardized (*z*-scored) variable.


<br />


## Using a Built-In Function

We can also use the `lm.ridge()` function from the **MASS** package to fit a ridge regression. This function uses a formula based on variables from a data frame in the same fashion as the `lm()` function. It also takes the argument `lambda=` which specifies the values of *d* to use in the penalty term.

```{r}
# Create data frame for use in lm.ridge()
z_data = z_eeo %>%
  data.frame()

# Load MASS library
library(MASS)

# Fit ridge regression
ridge_1 = lm.ridge(achievement ~ -1 + faculty + peer + school, data = z_data, lambda = 0.1)

# View coefficients
tidy(ridge_1)
```


<br />


## Comparison to the OLS Coefficients

Fitting a standardized OLS model to the data, we find:

```{r}
# Convert the scaled data to a data frame
z_data = z_eeo %>%
  data.frame()

# Fit standardized OLS model
lm.1 = lm(achievement ~ faculty + peer + school - 1, data = z_data)
coef(lm.1)
```

Comparing these coefficients to the coefficients from the ridge regression:

```{r echo=FALSE}
tab_01 = data.frame(
  Predictor = c("Faculty", "Peer", "School"),
  OLS = coef(lm.1),
  Ridge = b[,1]
)

kable(tab_01, 
      digits = 3, 
      row.names = FALSE, 
      col.names = c("Predictor", "$d=0$", "$d=0.1$"),
      caption = "Comparison of the coefficients from the OLS (d=0) and ridge regression (d=0.1) based on the standardized data.",
      
      ) %>%
  kable_classic_2(full_width = TRUE)
```

Based on this comparison, the ridge regression has "shrunk" the estimate of each coefficient toward zero. Remember, the larger the value of $d$, the more the coefficient estimates will shrink toward 0. The table below shows the coefficient estimates for four different values of $d$.

```{r echo=FALSE}
tab_02 = data.frame(
  B = c("Faculty", "Peer", "School"),
  x1 = coef(lm.1),
  x4 = coef(MASS::lm.ridge(achievement ~ faculty + peer + school - 1, data = z_data, lambda = 0.01)),
  x2 = coef(MASS::lm.ridge(achievement ~ faculty + peer + school - 1, data = z_data, lambda = 0.1)),
  x3 = coef(MASS::lm.ridge(achievement ~ faculty + peer + school - 1, data = z_data, lambda = 0.5))
)

kable(tab_02, 
      digits = 3, 
      row.names = FALSE, 
      col.names = c("Predictor", "$d=0$", "$d=0.01$", "$d=0.1$", "$d=0.5$"), 
      caption = "Comparison of the coefficients from the OLS (d=0) and three ridge regressions (d=0.01, d=0.1, and d=0.5) based on the standardized data."
      )  %>%
  kable_classic_2(full_width = TRUE)
```

<br />


## Choosing *d*

In practice you need to specify the *d* value to use in the ridge regression. Ideally you want to choose a value for *d* that:

- Introduces the least amount of bias possible, while also
- Obtaining better sampling variances.

This is an impossible task without knowing the true values of the coefficients (i.e., the $\beta$ values). There are, however, some empirical methods to help us in this endeavor.


### Ridge Trace

The plot of the *ridge trace* displays the ridge regression coefficients as a function of *d*. Examining this plot, the analyst can use it to pick a *d* value. To do this, pick the smallest value for *d* that produces stable regression coefficients. Here we examine the values of *d* where $d = \{ 0,0.001,0.002,0.003,\ldots,100\}$. 

To create this plot, we fit a ridge regression that includes a sequence of values in the `lambda=` argument of the `lm.ridge()` function. Then we use the `tidy()` function from the **broom** library to summarize the output from this model. This output includes the coefficient estimates for each of the *d* values in our sequence. We can then create a line plot of the coefficient values versus the *d* values for each predictor.

```{r fig.width=8, fig.height=6, out.width='70%', fig.cap="Ridge plot showing the size of the standardized regression coefficients for d values between 0 (OLS) and 100."}
# Fit ridge model across several lambda values
ridge_models = lm.ridge(achievement ~ -1 + faculty + peer + school, data = z_data, lambda = seq(from = 0, to = 100, by = 0.001))

# Get tidy() output
out = tidy(ridge_models)
out

# Ridge trace
ggplot(data = out, aes(x = lambda, y = estimate)) +
  geom_line(aes(group = term, color = term)) +
  theme_bw() +
  xlab("d value") +
  ylab("Coefficient estimate") +
  ggsci::scale_color_d3(name = "Predictor")
```

We want to find the *d* value where the lines begin to flatten out; where the coefficients are no longer changing value. This is difficult to ascertain, but somewhere around $d=50$, there doesn't seem to be a lot of change in the coefficients. This suggests that a *d* value around 50 would produce stable coefficient estimates. 

It turns out that the ridge plot often results in selecting a *d* value that is too high. The GCV (generalized cross-validation) measure is another way to select the *d* value. We want to find the *d* value associated with the smallest GCV measure.

<aside>
Unfortunately, the selection of *d* from the ridge trace plot is based on using the data to both fit the model and evaluate the model. This tends to overfit to the data we used. In other words, a *d* of 50 may work well for this particular sample, but may not be the most generalizable value of *d*. Using the GCV measure to pick *d* uses the method of cross-validation and results in a more generalizable value of *d*. We will examine this methodology in a future set of notes. 
</aside>

```{r}
# Find d value associated with smallest GCV value
out %>%
  filter(GCV == min(GCV))
```

Based on this selection criteria, we will choose a *d* value of 22.436.


```{r}
# Re-fit ridge regression
ridge_3 = lm.ridge(achievement ~ -1 + faculty + peer + school, data = z_data, lambda = 22.436)

# View coefficients
tidy(ridge_3)
```

Based on using $d=22.436$, the fitted ridge regression model is:

$$
\hat{\mathrm{Achievement}}^{\star}_i = 0.115(\mathrm{Faculty}^{\star}_i) + 0.172(\mathrm{Peer}^{\star}_i) - 0.010(\mathrm{School}^{\star}_i)
$$

<br />

## Estimating Bias

Recall that the ridge regression produces biased estimates such that $\mathbb{E}(\hat{\beta})\neq\beta$. The amount of bias in the coefficient estimates is defined as:

$$
\mathrm{Bias}(\hat{\boldsymbol{\beta}}_{\mathrm{Ridge}}) = -d(\mathbf{X}^{\intercal}\mathbf{X}+d\mathbf{I})^{-1}\boldsymbol{\beta}
$$
where $\boldsymbol{\beta}$ are the coefficients from the standardized regression model.

Remember that $d=0$ was the OLS model. In that case,

$$
\begin{split}
\mathrm{Bias}(\hat{\boldsymbol{\beta}}_{\mathrm{OLS}}) &= 0(\mathbf{X}^{\intercal}\mathbf{X}+0\mathbf{I})^{-1}\boldsymbol{\beta} \\[0.5em]
&= 0
\end{split}
$$

There is no bias in the OLS coefficients. Of course, as *d* increases, the bias in the coefficient estimates will also increase. We can use our sample data to estimate the bias, it is likely a poor estimate, as to obtain the bias we really need to know the true $\beta$-parameters (which of course we do not know). 

```{r}
# OLS estimates
b_ols = solve(t(X) %*% X) %*% t(X) %*% Y

# Compute dI
dI = 22.436*diag(3)

# Estimate bias in ridge regression coefficients
-22.436 * solve(t(X) %*% X + dI) %*% b_ols
```
It looks like the faculty and peer coefficients are biased downward and the school coefficient is biased upward in our ridge regression model. We could also see this in the ridge trace plot we created earlier.

```{r fig.width=8, fig.height=6, out.width='70%', fig.cap="Ridge plot showing the size of the standardized regression coefficients for d values between 0 (OLS) and 100. The vertical dotted line is shown at x=22.436, the value of d that has the minimum GCV value."}
# Ridge trace
ggplot(data = out, aes(x = lambda, y = estimate)) +
  geom_line(aes(group = term, color = term)) +
  geom_vline(xintercept = 22.436, linetype = "dotted") +
  theme_bw() +
  xlab("d value") +
  ylab("Coefficient estimate") +
  ggsci::scale_color_d3(name = "Predictor")
```

In this plot, the estimates for the peer and faculty coefficients are being shrunken downward from the OLS estimates (at $d=0$), and the school coefficient is being "shrunken" upward (closer to 0). Moreover, the bias is simply the difference between the OLS estimates and the ridge coefficient estimates.

```{r}
# Difference b/w OLS and ridge coefficients
tidy(ridge_3)$estimate - b_ols
```


<br />


## Sampling Variances, Standard Errors, and Confidence Intervals

In theory it is possible to obtain the sampling variances for the ridge regression coefficients using matrix algebra:

$$
\mathrm{Var}(\widetilde{\boldsymbol{\beta}}) = \sigma^2_{\epsilon}(\mathbf{X}^\intercal \mathbf{X} + d\mathbf{I})^{-1}\mathbf{X}^\intercal \mathbf{X} (\mathbf{X}^\intercal \mathbf{X} + d\mathbf{I})^{-1}
$$

where $\sigma^2_{\epsilon}$ is the the error variance estimated from the standardized OLS model.

```{r}
# Fit standardized model to obtain sigma^2_epsilon
glance(lm(achievement ~ -1 + faculty + peer + school, data = z_data))

# Compute sigma^2_epsilon
mse = 0.9041214 ^ 2

# Compute variance-covariance matrix of ridge estimates
W = solve(t(X) %*% X + 22.436*diag(3))
var_b = mse * W %*% t(X) %*% X %*% W

# Compute SEs
sqrt(diag(var_b))
```


Comparing these SEs to the SEs from the OLS regression:

```{r echo=FALSE}
tab_01 = data.frame(
  Predictor = c("Faculty", "Peer", "School"),
  OLS = tidy(lm(achievement ~ -1 + faculty + peer + school, data = z_data))$std.error,
  Ridge = sqrt(diag(var_b))
)

kable(tab_01, 
      digits = 3, 
      row.names = FALSE, 
      col.names = c("Predictor", "$d=0$", "$d=0.1$"),
      caption = "Comparison of the standard errors from the OLS (d=0) and ridge regression (d=0.1) based on the standardized data.",
      
      ) %>%
  kable_classic_2(full_width = TRUE)
```

Based on this comparison, we can see that the standard errors from the ridge regression are quite a bit smaller than those from the OLS. We can then use the estimates and SEs to compute *t*- and *p*-values, and confidence intervals. Here we only do it for the predictor of interest from our RQ, but one could do it for all the predictors.

```{r}
# Compute t-value for school predictor
-0.09995221	 / 0.04083782 

# Compute p-value
pt(-2.44754, df = 66) * 2

# Compute CI
-0.09995221 - qt(p = 0.975, df = 66) * 0.04083782 
-0.09995221  + qt(p = 0.975, df = 66) * 0.04083782 
```

This suggests that after controlling for peer influence and faculty credential, there does seem to be an effect of school facilities on student achievement. The 95% CI suggests that the true partial effect of school facilities is between $-0.181$ and $-0.184$.

<br />

### Bias--Variance Tradeoff: Revisited

Now that we have seen how the bias and the sampling variance are calculated, we can study these formulae to understand why there is a bias--variance tradeoff.

$$
\begin{split}
\mathrm{Bias}(\hat{\boldsymbol{\beta}}_{\mathrm{Ridge}}) &= -d(\mathbf{X}^{\intercal}\mathbf{X}+d\mathbf{I})^{-1}\boldsymbol{\beta} \\[0.5em]
\mathrm{Var}(\hat{\boldsymbol{\beta}}_{\mathrm{Ridge}}) &= \sigma^2_{\epsilon}(\mathbf{X}^\intercal \mathbf{X} + d\mathbf{I})^{-1}\mathbf{X}^\intercal \mathbf{X} (\mathbf{X}^\intercal \mathbf{X} + d\mathbf{I})^{-1}
\end{split}

$$

Examining these two formulas, we can see that as *d* increases, the bias also increases. At the same time, increasing *d* in the formula for sampling variance will decrease. This. in a nutshell, is the bias--variance tradeoff. Decreasing one of these properties tends to increase the other. The key is to find a value of *d* that minimally increases bias to maximally decrease the sampling variances.



<br />

### MSE of the Estimate

Recall that the sampling variation for the coefficients is measured via the variance. Mathematically, the variance estimate of a coefficient is defined as the expectation of the squared difference between the parameter value and the estimate:

$$
\sigma^2_{\hat{\beta}} = \mathbb{E}\bigg[(\hat{\beta} - \beta)^2\bigg]
$$

Using rules of expectations, we can re-write this as:

$$
\begin{split}
\sigma^2_{\hat{\beta}} &= \mathbb{E}\bigg[\bigg(\hat{\beta} - \mathbb{E}\big[\beta\big]\bigg)^2\bigg] + \bigg(\mathbb{E}\big[\hat{\beta}-\beta\big]\bigg)^2
\end{split}
$$

The first term in this sum represents the variance in $\hat{\beta}$ and the second term is the squared amount of bias in $\hat{\beta}$. As a sum,

$$
\sigma^2_{\hat{\beta}} = \mathrm{Var}(\hat{\beta}) + \mathrm{Bias(\hat{\beta})}^2
$$
In the OLS model, the bias of the estimate is 0, and this reduces to the variance of the estimate. In ridge regression, the bias term is not zero. The bias--variance tradeoff says that by increasing bias, we will decrease the variance. So while the second term will get bigger. the first will get smaller. The hope is that overall we can reduce the size of the MSE. However, since the MSE use the square of the bias, we have to be careful that we don't increase bias too much, or it will be counterproductive.

One major issue in estimating the sampling variation for a coefficient in ridge regression is that the size of the variance estimate is now dependent on both the variance and the amount of bias. This means we need to know how much bias there is to get a true accounting of the sampling variation. As @Goeman:2018 notes,

> Unfortunately, in most applications of penalized regression it is impossible to obtain a sufficiently precise estimate of the bias...calculations can only give an assessment of the variance of the estimates. Reliable estimates of the bias are only available if reliable unbiased estimates are available, which is typically not the case in situations in which penalized estimates are used.

He goes on to make it clear why most programs do not report SEs for the coefficients:

> Reporting a standard error of a penalized estimate therefore tells only part of the story. It can give a mistaken impression of great precision, completely ignoring the inaccuracy caused by the bias. It is certainly a mistake to make confidence statements that are only based on an assessment of the variance of the estimates.

<br />



# Least Absolute Shrinkage and Selection Operator (LASSO)

One disadvantage to ridge regression is that although the coefficients are "shrunk", they are not set to 0 (unless $d=\infty$). In some cases this is fine, but in many applications it may be of interest to find the subset of correlated predictors that are useful. This is especially true when your data have a large number of predictors and a limited number of cases (the so-called "large *p*, small *n* problems). In such cases, the LASSO [@Tibshirani:1996] is an alternative to ridge regression. Similar to ridge regression, the LASSO finds the coefficients that minimize a penalized sum of squares, namely:

$$
\mathrm{SSE}_{\mathrm{Penalized}} = \sum_{i=1}^n \bigg(y_i - \hat y_i\bigg)^2 + d \sum_{j=1}^p \vert \beta_j \vert
$$

If we compare this to ridge regression, the only difference is in the penalty term. Namely, the penalty term for ridge regression was based on the sum of the squared beta terms while that for the LASSO is based on the sum of the absolute value for the beta terms.

<aside>
In statistical nomenclature, the LASSO uses an L1 penalty and ridge regression uses an L2 penalty. This terminology is based on the norm (magnitude) of the vector of beta coefficients. The L1 norm is the sum of the absolute values (Manhattan distance) and the L2 norm is the sum of the squared values (Euclidean distance).
</aside>

Similar to ridge regression, the penalty term has the effect of biasing some coefficients by shrinking them toward zero. The LASSO, however, will shrink some coefficients to 0; effectively removing them from the model. We can also use the `glmnet()` function from the **glmnet** package to fit a LASSO. To fit a ridge regression we provide the following arguments to `glmnet()`:

- `x=`: A matrix of the predictors
- `y=`: A vector of the outcome
- `alpha=1`: This fits a LASSO
- `lambda=`: This sets a modified $d$ value which is equal to $\frac{d}{n}$
- `intercept=FALSE`: We will include this argument to drop the intercept term; appropriate for standardized regression.
- `standardize=`: Here we set this to `FALSE` since we pre-standardized the predictors and outcome. The default is `standardize=TRUE`.


<aside>
The `glmnet()` function optimizes ove the MSE + Penalty rather than the SSE + Penalty. Because of this, we scale the value of *d* that we put in the `lambda=` argument by dividing it by *n*. This function can also be used to fit a ridge regression by setting the `alpha=` argument to `0`.
</aside>




```{r}
# Load glmnet package
library(glmnet)

# Create matrix of standardized predictors
X = scale(eeo[ , c("faculty", "peer", "school")]) %>% 
  data.matrix()

# Create matrix of standardized outcome
Y = scale(eeo[ , c("achievement")]) %>% 
  data.matrix()


# Fit ridge regression
lasso_1 = glmnet(
  x = X,
  y = Y,
  alpha = 1,
  lambda = 22.436 / 70,
  intercept = FALSE,
  standardize = FALSE
  )

# Show coefficients
tidy(lasso_1)
```

Based on using $d=3.5$, the fitted LASSO model is:

$$
\hat{\mathrm{Achievement}}^{\star}_i = 0.114(\mathrm{Peer}^{\star}_i)
$$

Note that the coefficients for faculty credentials and school facilities were both shrunk to zero! Computing standard errors for LASSO coefficients are problematic in the same way that computing SEs are problematic for the ridge regression; namely we have no good estimate of the true amount of bias.

Lastly, we note that, as in ridge regression, selecting a good value of *d* for the LASSO is critical. Here we just used the value we had obtained from the trace plot of the ridge regression. We will return to this when we talk about cross-validation in a future set of notes.

<aside>
A generalization of the ridge and lasso penalties, introduced by @Zou:2005 is called the *elastic net*. This combines the two penalties and subsequently minimizes a double-penalized sum of squares, namely: $\mathrm{SSE}_{\mathrm{Penalized}} = \sum_{i=1}^n \bigg(y_i - \hat y_i\bigg)^2 + \bigg(\lambda_1 \sum^p_{j=1} \beta_j^2 + \lambda_2 \sum^p_{j=1} \vert \beta_j \vert \bigg)$.
</aside>

<br />


# References

